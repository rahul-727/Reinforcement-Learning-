{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcBqI3+g8chB/Ucm6mOIKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-727/Reinforcement-Learning-/blob/main/2348544_Lab4_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov Decision Process (MDP)"
      ],
      "metadata": {
        "id": "lX6N-Ghh2CVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "solving a Markov Decision Process (MDP) using policy iteration, which is a fundamental algorithm in reinforcement learning (RL). The purpose is to find the optimal policy and state value function for a given MDP, which describes a decision-making problem in a stochastic environment."
      ],
      "metadata": {
        "id": "G1z37LWY4UC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn2j6ZV2s-He"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP:\n",
        "  def __init__(self, states, actions, rewards, transition_probs, discount_factor=0.9):\n",
        "    \"\"\"we input States, actions, rewards, transition probabilities, and discount factor\n",
        "       Random Policy Initialization: Each state (s) is assigned a random action from the actions list\"\"\"\n",
        "    self.states = states # List of states\n",
        "    self.actions = actions # list of actions\n",
        "    self.rewards = rewards # dictionary where rewards [s][a] is the reward for taking action 'a' in state 's'\n",
        "    self.transition_probs = transition_probs # dictionary where transition_probs [s][a][s'] is the prob. of moving from state s to state s' with action a\n",
        "    self.discount_factor = discount_factor # discount factor(gamma) for future rewards\n",
        "    self.policy = {s: np.random.choice(actions) for s in states} # random initial policy\n",
        "\n",
        "  def policy_evaluation(self, theta=1e-6):\n",
        "    # the goal is to Compute the state-value function V(s) for a given policy.\n",
        "    V = {s: 0 for s in self.states} #initialuze state values with 0\n",
        "\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for s in self.states:\n",
        "        v = V[s]\n",
        "        a = self.policy[s]\n",
        "\n",
        "        # Bellman expectation equation\n",
        "        V[s] = sum(self.transition_probs[s][a][s_prime] * (self.rewards[s][a] + self.discount_factor * V[s_prime])\n",
        "        for s_prime in self.states)\n",
        "\n",
        "        delta = max(delta, abs(v - V[s])) # Check for convergence\n",
        "      if delta < theta:\n",
        "        break\n",
        "    return V\n",
        "\n",
        "  def policy_improvement(self, V):\n",
        "    #Goal: Improve the current policy by choosing actions that maximize the expected value for each state.\n",
        "\n",
        "    policy_stable = True\n",
        "    for s in self.states:\n",
        "      old_action = self.policy[s]\n",
        "\n",
        "      # Find the action that maximizes expected value\n",
        "      self.policy[s] = max(self.actions, key=lambda a:sum(\n",
        "          self.transition_probs[s][a][s_prime] * (self.rewards[s][a] + self.discount_factor * V[s_prime])\n",
        "          for s_prime in self.states))\n",
        "\n",
        "      if old_action != self.policy[s]:\n",
        "        policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "  def policy_iteration(self):\n",
        "    # Perform policy iteration : alternate between policy evaluation and improvement until the policy is stable(optimal)\n",
        "    while True:\n",
        "      V = self.policy_evaluation()\n",
        "      policy_stable = self.policy_improvement(V)\n",
        "      if policy_stable:\n",
        "        return self.policy, V\n"
      ],
      "metadata": {
        "id": "YvPPLsxptCfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MDP Components"
      ],
      "metadata": {
        "id": "kh2Hbx6x0pJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* States (states): A list of possible states in the environment (s1, s2,s3)\n",
        "* Actions (actions): A list of actions available in each state (a1, a2)\n",
        "* Rewards (rewards): A dictionary where rewards[s][a] gives the reward for taking action a in state s\n",
        "* Transition Probabilities (transition_probs):A dictionary where transition_probs[s][a][s'] is the probability of moving to state s' from state s after taking action a\n",
        "* Discount Factor (discount_factor): Denoted as gamma (Î³), it balances immediate and future rewards."
      ],
      "metadata": {
        "id": "JmO-eDob3Rqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = ['s1', 's2', 's3']\n",
        "actions = ['a1', 'a2']\n",
        "rewards = {\n",
        "    's1': {'a1': 5, 'a2': 10},\n",
        "    's2': {'a1': -1, 'a2': 2},\n",
        "    's3': {'a1': 0, 'a2': 0}\n",
        "}\n",
        "transition_probs = {\n",
        "    's1': {'a1': {'s1': 0.7, 's2': 0.3, 's3': 0.0}, 'a2': {'s1': 0.4, 's2': 0.6, 's3': 0.0}},\n",
        "    's2': {'a1': {'s1': 0.1, 's2': 0.6, 's3': 0.3}, 'a2': {'s1': 0.0, 's2': 0.9, 's3': 0.1}},\n",
        "    's3': {'a1': {'s1': 0.0, 's2': 0.0, 's3': 1.0}, 'a2': {'s1': 0.0, 's2': 0.0, 's3': 1.0}}\n",
        "}\n",
        "\n",
        "#create MDP instances and perform policy iteration\n",
        "mdp = MDP(states, actions, rewards, transition_probs, discount_factor=0.9)\n",
        "optimal_policy, optimal_value = mdp.policy_iteration()\n",
        "\n",
        "print(\"Optimal Policy:\", optimal_policy)\n",
        "print(\"Optimal State Values:\", optimal_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6G9l14l0mcw",
        "outputId": "cf4485ee-d3ae-4abd-c533-5565e5ebab21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy: {'s1': 'a2', 's2': 'a2', 's3': 'a1'}\n",
            "Optimal State Values: {'s1': 24.506574930441033, 's2': 10.526312442034197, 's3': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal action for the agent in each state is as follows:\n",
        "* In state s1, the best action is a2.\n",
        "* In state s2, the best action is a2.\n",
        "* In state s3, the best action is a1.\n",
        "* The agent's goal is to follow these actions in each state to maximize its long-term rewards.\n",
        "\n",
        "Optimal State Values\n",
        "\n",
        "These are the state-value functions for each state, which represent the expected total reward the agent can achieve starting from each state, following the optimal policy.\n",
        "* For state s1, the expected total reward is 24.51"
      ],
      "metadata": {
        "id": "qhyWzMbO4mDx"
      }
    }
  ]
}